{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import tarfile\n",
    "from torchvision.datasets.utils import download_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dowload the dataset\n",
    "dataset_url = \"http://files.fast.ai/data/cifar10.tgz\"\n",
    "download_url(dataset_url, '.')\n",
    "\n",
    "# Extract from archive\n",
    "with tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n",
    "    tar.extractall(path='./data')\n",
    "        \n",
    "data_dir = './data/cifar10'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tt\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "#data_dir = '../CIFAR10_CNN/data/cifar10'\n",
    "\n",
    "# Data transforms (normalization & data augmentation)\n",
    "stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "train_tfms = tt.Compose([tt.RandomCrop(32, padding=4, padding_mode='reflect'), \n",
    "                         tt.RandomHorizontalFlip(), \n",
    "                         tt.ToTensor(), \n",
    "                         tt.Normalize(*stats,inplace=True)])\n",
    "valid_tfms = tt.Compose([tt.ToTensor(), tt.Normalize(*stats)])\n",
    "\n",
    "# PyTorch datasets\n",
    "train_ds = ImageFolder(data_dir+'/train', train_tfms)\n",
    "valid_ds = ImageFolder(data_dir+'/test', valid_tfms)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "# PyTorch data loaders\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "# Model\n",
    "def conv_2d(ni, nf, stride=1, ks=3):\n",
    "    return nn.Conv2d(in_channels=ni, out_channels=nf, \n",
    "                     kernel_size=ks, stride=stride, \n",
    "                     padding=ks//2, bias=False)\n",
    "\n",
    "def bn_relu_conv(ni, nf):\n",
    "    return nn.Sequential(nn.BatchNorm2d(ni), \n",
    "                         nn.ReLU(inplace=True), \n",
    "                         conv_2d(ni, nf))\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, ni, nf, stride=1):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm2d(ni)\n",
    "        self.conv1 = conv_2d(ni, nf, stride)\n",
    "        self.conv2 = bn_relu_conv(nf, nf)\n",
    "        self.shortcut = lambda x: x\n",
    "        if ni != nf:\n",
    "            self.shortcut = conv_2d(ni, nf, stride, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn(x), inplace=True)\n",
    "        r = self.shortcut(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x) * 0.2\n",
    "        return x.add_(r)\n",
    "\n",
    "def make_group(N, ni, nf, stride):\n",
    "    start = ResidualBlock(ni, nf, stride)\n",
    "    rest = [ResidualBlock(nf, nf) for j in range(1, N)]\n",
    "    return [start] + rest\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, x): return x.view(x.size(0), -1)\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, n_groups, N, n_classes, k=1, n_start=16):\n",
    "        super().__init__()      \n",
    "        # Increase channels to n_start using conv layer\n",
    "        layers = [conv_2d(3, n_start)]\n",
    "        n_channels = [n_start]\n",
    "        \n",
    "        # Add groups of BasicBlock(increase channels & downsample)\n",
    "        for i in range(n_groups):\n",
    "            n_channels.append(n_start*(2**i)*k)\n",
    "            stride = 2 if i>0 else 1\n",
    "            layers += make_group(N, n_channels[i], \n",
    "                                 n_channels[i+1], stride)\n",
    "        \n",
    "        # Pool, flatten & add linear layer for classification\n",
    "        layers += [nn.BatchNorm2d(n_channels[3]), \n",
    "                   nn.ReLU(inplace=True), \n",
    "                   nn.AdaptiveAvgPool2d(1), \n",
    "                   Flatten(), \n",
    "                   nn.Linear(n_channels[3], n_classes)]\n",
    "        \n",
    "        self.features = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x): return self.features(x)\n",
    "    \n",
    "def wrn_22(): \n",
    "    return WideResNet(n_groups=3, N=3, n_classes=10, k=6)\n",
    "\n",
    "model = wrn_22()\n",
    "\n",
    "from fastai.basic_data import DataBunch\n",
    "from fastai.train import Learner\n",
    "from fastai.metrics import accuracy\n",
    "\n",
    "data = DataBunch.create(train_ds, valid_ds, bs=batch_size, path='./data/cifar10')\n",
    "learner = Learner(data, model, loss_func=F.cross_entropy, metrics=[accuracy])\n",
    "learner.clip = 0.1 # gradient is clipped to be in range of [-0.1, 0.1]\n",
    "\n",
    "# Find best learning rate\n",
    "learner.lr_find()\n",
    "learner.recorder.plot() #Â select lr with largest negative gradient (about 5e-3)\n",
    "\n",
    "# Training\n",
    "epochs = 20\n",
    "lr = 5e-3\n",
    "wd = 1e-4\n",
    "\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "learner.fit_one_cycle(epochs, lr, wd=wd) # wd is the lambda in l2 regularization\n",
    "t1 = time.time()\n",
    "\n",
    "print('time: ', t1-t0)\n",
    "\n",
    "# training process diagnostics\n",
    "learner.recorder.plot_lr()\n",
    "learner.recorder.plot_losses()\n",
    "learner.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cifar10-wrn22.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink('cifar10-wrn22.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "\t'arch': str(model),\n",
    "    'num_epochs': epochs,\n",
    "    'opt_func': 'Adam',\n",
    "    'batch_size': batch_size,\n",
    "    'lr': lr,\n",
    "    'scheduler': 'one-cycle',\n",
    "    'weight_decay': wd,\n",
    "    'grad_clip': learner.clip\n",
    "}\n",
    "\n",
    "metrics = {\n",
    "    'train_loss': 0.152067,\n",
    "\t'val_acc': 0.916,\n",
    "\t'val_loss': 0.260238,\n",
    "\t'time': t1-t0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('hyper_params.json', 'w') as fp:\n",
    "    json.dump(hyper_params, fp)\n",
    "\n",
    "with open('metrics.json', 'w') as fp:\n",
    "    json.dump(metrics, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink('metrics.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink('hyper_params.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
